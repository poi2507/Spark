{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 완벽 가이드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame과 SQL을 사용해 클러스터, 스파크 애플리케이션 그리고 구조적 API를 살펴본다. <br>\n",
    "이 과정에서 스파크의 핵심 용어와 개념을 접하고 사용법을 익힌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 스파크의 기본 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터를 처리해야 하는 경우 한대의 컴퓨터로 수행하기가 힘들다.**<br>\n",
    "컴퓨터 클러스터는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 만든다. 하지만 컴퓨터 클러스터를 구성하는 것만으로는 부족하며 클러스에서 작업을 조율할 수 있는 프레임워크가 필요하다. \n",
    "스파크는 바로 그런 역할을 하는 **프레임워크**이며, 스파크 클러스터의 데이터 처리 작업을 **관리**하고 **조율**한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 스파크 애플리케이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크 애플리케이션은 **드라이버** 프로세스와 다수의 **익스큐터** 프로세스로 구성된다.\n",
    "\n",
    "**드라이버 프로세스**\n",
    "\n",
    "- 클러스터 노드 중 하나에서 실행(main 함수를 실행)\n",
    "- 스파크 애플리케이션 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답\n",
    "- 익스큐터 프로세스의 작업과 관련되 분석, 배포, 스케줄링 역활 수행\n",
    "\n",
    "**익스큐터 프로세스**\n",
    "- 드라이버가 할당한 코드를 실행\n",
    "- 진행 상황을 다시 드라이버 노드에 보고\n",
    "\n",
    "\n",
    "<img src=\"IMG_4AFAC37A065A-1.jpeg\"  width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크 애플리케이션을 이해하기 핵심사항\n",
    "* 스파크는 사용 가능한 자원을 파악하기 위해 클러스터 매니저를 사용한다\n",
    "* 드라이버 프로세스는 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터에서 실행할 책임이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 스파크의 다양한 언어 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스칼라 : 스파크는 스칼라로 개발되어 있으므로 스칼라가 스파크의 '기본'언어이다.\n",
    "* 자바 : 스파크가 스칼라로 개발되어 있지만, 스파크 창시자들은 자바를 이용해 스파크 코드를 작성할 수 있도록 심혈을 기울였다.\n",
    "* 파이썬 : 파이썬은 스칼라가 지원하는 거의 모든 구조를 지원한다.\n",
    "* SQL : 스파크는 ANSI SQL:2003표준 중 일부를 지원한다. 분석가나 비프로그래머도 SQL을 이용해 스파크의 강력한 빅데이터 처리 능력을 쉽게 활용할 수 있다.\n",
    "* R : 스파크에는 일반적으로 사용하는 두 개의 R라이브러리가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_D39CFF2F4758-1.jpeg\"  width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 스파크 시작하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 스파크 애플리케이션을 개발하려면 사용자 명령과 데이터를 스파크 애플리케이션에 전송하는 방법을 알아야 한다.\n",
    "SparkSeccion을 생성하면서 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크 애플리케이션은 SparkSession이라 불리는 드라이버 프로세스로 제어한다. SparkSession인스턴스는 사용자가 정의한 처리 명령을 클러스터에서 실행한다. 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방금 생성한 DataFrame은 한 개의 컬럼과 1000개의 로우로 구성되며 각 로우에는 0부터 999까지의 값이 할당되어 있다. 이 숫자들은 **분산 컬렉션**\n",
    "을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame은 가장 대표적인 구조적 API이다. DataFrame은 테이블의 데이터를 로우와 컬럼으로 단순하게 표현한다. 컬럼과 컬럼의 타임을 정의한 목을을 스키마라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"IMG_F2BAD5335524-1.jpeg\"  width=\"700\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 파티션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할한다. <br>\n",
    "DataFrame의 파티션은 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타낸다.<br>\n",
    "DataFrame을 사용하면 파티션을 수동 혹은 개별적으로 처리할 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 트랜스포메이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크의 핵심 데이터 구조는 **불변성**을 가지는 것이다. 즉, 한번 생성하면 변경할 수 없다.<br>\n",
    "DataFrame을 '변경'하려면 원하는 변경 방볍을 스파크에세 알려줘야 한다. 이때 사용하는 명령을 **트렌스포메이션**이라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트렌스포메이션에는 두 가지 유형이 있다\n",
    "* 좁은 의존성 : 각 입력 파티션이 하나의 출력 파이션에만 영향을 준다.\n",
    "<br>\n",
    "<img src=\"IMG_FF15FDF62B1E-1.jpeg\" align=\"left\"  width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 넓은 의존성 : 하나의 입력 파티션이 여러 출력 파티션에 영향을 준다. \n",
    "\n",
    "\n",
    "<img src=\"IMG_475F8AC8DAA7-1.jpeg\" align=\"left\"  width=\"500\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크가 클러스터에서 파티션을 교환하는 **셔플**이라는 단어를 자주 들었을 거다. 좁은 트랜스포메이션을 사용하면 스파크에서 **파이프라이닝**을 자동으로 수행한다.<br>\n",
    "스파크는 셔플의 결과를 디스크에 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 지연연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지연연산은 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미한다. 스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 **실행계획**을 생성하게 된다. <br>\n",
    "스파크 코드를 실행하는 마지막 순간까지 대가하다가 원형 DataFrame 트렌스포메이션을 간결한 물리적 실행 계획으로 컴파일한다.<br>\n",
    "스파크는 이 과정을 거치며 전체 데이터 흐름을 최적화하는 엄청난 강점을 가지고 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 액션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "divisBy2.count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자는 트랜스포메이션을 사용해 논리적 실행 계획을 세울 수 있다. 하지만 실제 연산을 수행하려면 액션 명령을 내려야한다.\n",
    "* 콘솔에서 데이터를 보는 액션\n",
    "* 각 언어로 된 네이티브 객체에 데이터를 모으는 액션\n",
    "* 출력 데이터소스에 저장하는 액션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "액션을 지정하면 스파크잡이 시작된다. 스파크 잡은 필터(좁은 트랜스포메이션)를 수행 한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)를 한다. 그리고 각 언어에 적합한 네이티브 객체에 결과를 모은다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 스파크 UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크 UI는 드라이버 노드의 4040포트로 접속할 수 있다. -> http://localhost:4040 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 종합 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 다양한 데이터소스를 지원한다. 데이터는 SparkSession의 DataFrameReader 클래스를 사용해서 읽는다. 이때 특정 파일 포맷과 몇 가지 옵션을 함께 설정한다.<br>\n",
    "ex) 스키마 정보를 알아내는 **스키마 추론** 기능, 파일의 첫 로우를 헤더로 지정하는 옵션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**스파크는 스키마 정보를 얻기 위해 데이터를 조금 읽는다.** 그리고 해당 로우의 데이터 타입을 스파크 데이터 타입에 맞게 분석한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark \\\n",
    "    .read \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .option('header', 'true') \\\n",
    "    .csv('./data/flight-data/csv/2015-summary.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라와 파이썬에서 사용하는 DataFrame은 불특정 다수의 로우와 컬럼을 가진다. 로우의 수를 알 수 없는 이유는 데이터를 읽는 과정이 이연 연산 형태의 트랜스포메이션이기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_7782B6B3AFC0-1.jpeg\"   width=\"500\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    트랜스포메이션을 추가로 지정, 정수 데이터 타입인 count 컬럼을 기준으로 데이터를 정렬한다\n",
    " <img src=\"IMG_3BED1F818BC3-1.jpeg\"  width=\"500\"/>\n",
    " \n",
    " sort메서드는 단지 트랜스포메이션이기 때문에 호출 시 데이터에 아무런 변화가 일어나지 않는다. 스파크는 실핼 계획을 만들고 검토하여 클러스터에서 처리할 방법을 알아낸다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#18 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#18 ASC NULLS FIRST, 200), true, [id=#32]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#16,ORIGIN_COUNTRY_NAME#17,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/taewoong/Documents/coding/Spark_practice/data/flight-data/csv/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에서는 각 줄의 첫 번째 키워드(Sort, Exchange, FileScan)를 주목한다. 특정 컬럼을 다른 컬럼과 비교하는 sort메서드가 넓은 트랜스포메이션으로 동작하는 것을 볼 수 있다.<br>\n",
    "실행 계획은 디버깅과 스파크의 실랭 과정을 이해하는 데 도움을 주는 도구일 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트랜스포메이션 실행 계획을 시작하기 위해 액션을 호출한다. 액션을 실행하려면 몇가지 설정이 필요하다. 스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션을 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",'5') # 본 예시에서는 파티션의 수를 5로 설정해 파티션 수를 줄인다.\n",
    "flightData2015.sort(\"count\").take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_60720240CEF1-1.jpeg\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트랜스포메이션의 논리적 실행 계획은 DataFrame의 계보를 정의한다. 스파크는 계보를 통해 입력 데이터에 수행한 연산을 전체 파티션에서 어떻게 재연산하는지 알 수 있다. 이 기능은 스파크의 프로그래밍 모델인 함수형 프로그래밍의 핵심이다. <br>\n",
    "사용자는 물리적 데이터를 직접 다루지 않는다. 대신 앞서 설정한 셔플 파티션 파라미터와 같은 속성으로 물리적 실행 특성을 제어한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.1 DataFrame과 SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame과 SQL을 사용하는 복잡한 작업을 수행해보자. <br>\n",
    "스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시 테이블)로 등록한 후 SQL 쿼리를 사용할 수 있다. 스파크는 SQL쿼리를 DataFrame 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능 차이는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), true, [id=#67]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/taewoong/Documents/coding/Spark_practice/data/flight-data/csv/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), true, [id=#86]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/taewoong/Documents/coding/Spark_practice/data/flight-data/csv/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\"\n",
    "sqlWay = spark.sql(query)\n",
    "dataFramesWay = flightData2015.groupBy('DEST_COUNTRY_NAME').count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFramesWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예제는 특정 위치를 왕래하는 최대 비행 횟수를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예제는 상위 5개 도착 국가를 찾아내는 코드를 실행해보겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame 방식**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015.groupby(\"DEST_COUNTRY_NAME\").sum(\"COUNT\").withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행 계획은 트랜스포메이션의 **지향적 비순환 그래프**이며 액션이 호출되면 결과를 만들어냅니다. 그리고 지향성 비순환 그래프의 각 단계는 불변성을 가진 신규 DataFrame을 생성한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMG_4320A3EEEC0F-1.jpeg\" width =\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 단계에서는 데이터를 읽는다. 데이터를 읽는 DataFrame은 이미 이전 예제에서 정의했다. 다시 한번 강조하면 스파크는 해당 DataFrame이나 자신의 원본 DataFrame에 액션이 호출되기 전까지 데이터를 읽지 않는다.<br><br>\n",
    "두 번째 단계에서는 데이터를 그룹화 한다. groupBy 메서드가 호출되면 최종적으로 그룹화된 DataFrame을 지칭하는 이름을 가진 RelationalGroupedDataset을 반환한다. 기본적으로 키 혹은 키셋을 기준으로 그룹을 만들고 각 키에 대한 집계를 수행<br><br>\n",
    "세 번째 단계에서는 집계 유형을 지정하기 위해 컬럼 표현식이나 컬럼명을 인수로 사용하는 sum메서드를 사용한다. sum 메서드는 새로운 스키마 정보를 가지는 별도의 DataFrame을 생성한다. 신규 스키마에서 새로 만들어진 각 컬럼의 데이터 타입 정보가 들어있다. <br><br>\n",
    "네 번째 단계에서는 컬럼명을 변경한다. 이름을 변경하기 위해 withColumnRenamed 메서드에 원본 컬럼명과 신규 컬럼명을 인수로 지정한다.<br><br>\n",
    "다섯 번째 단계에서 데이터를 정렬한다. 역순으로 정렬하기 위해 desc 함수를 임포드 한다.desc 함수는 문자열이 아닌 column객체를 반환한다.<br><br>\n",
    "여섯 번째 단계에서는 limit 메서드로 반환 결과의 수를 제한한다.<br><br>\n",
    "마지막 단계에서는 액션을 수행한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#113L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#16,destination_total#113L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(cast(COUNT#18 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), true, [id=#209]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(cast(COUNT#18 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/taewoong/Documents/coding/Spark_practice/data/flight-data/csv/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.groupby(\"DEST_COUNTRY_NAME\").sum(\"COUNT\").withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
